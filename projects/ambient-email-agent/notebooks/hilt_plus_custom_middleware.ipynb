{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86bef512",
   "metadata": {},
   "source": [
    "###  Complete End-to-End LangGraph HITL with Total Custom Freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27083cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "from langchain.agents import create_agent\n",
    "from ambient_email_agent.tools.base import write_email, schedule_meeting, check_calendar_availability, Question\n",
    "from ambient_email_agent.custom_middleware.custom_interrupt_middleware import CustomInterruptMiddleware\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM for use with router / structured output\n",
    "from langchain.chat_models import init_chat_model\n",
    "model_gemini_flash = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", timeout=30, temperature=0)\n",
    "model_llama_groq = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\", timeout=30, temperature=0)\n",
    "model_gpt_4o_mini = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambient_email_agent.prompts import agent_system_prompt_hitl,HITL_TOOLS_PROMPT,default_background,default_response_preferences,default_cal_preferences\n",
    "system_prompt_for_email_writer_agent=agent_system_prompt_hitl.format(tools_prompt=HITL_TOOLS_PROMPT, \n",
    "                                                                     background=default_background,\n",
    "                                                                     response_preferences=default_response_preferences, \n",
    "                                                                     cal_preferences=default_cal_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1279570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "Markdown(system_prompt_for_email_writer_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36706d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambient_email_agent.custom_middleware.interrupt_request_payload_builders.question_email_tool_payload import build_question_payload\n",
    "from ambient_email_agent.custom_middleware.interrupt_request_payload_builders.schedule_email_tool_payload import build_schedule_meeting_payload\n",
    "from ambient_email_agent.custom_middleware.interrupt_request_payload_builders.write_email_tool_payload import build_write_email_payload\n",
    "from ambient_email_agent.custom_middleware.interrupt_response_handlers.question_email_intterup_response_handler import process_question_response\n",
    "from ambient_email_agent.custom_middleware.interrupt_response_handlers.schedule_email_interrupt_response_handler import process_schedule_meeting_response\n",
    "from ambient_email_agent.custom_middleware.interrupt_response_handlers.write_tool_interrupt_response_handler import process_write_email_response\n",
    "from ambient_email_agent.custom_middleware.tool_interrupt_configuration import ToolInterruptConfig\n",
    "\n",
    "\n",
    "tool_configs = {\n",
    "    \"write_email\": ToolInterruptConfig(\n",
    "        payload_builder=build_write_email_payload,\n",
    "        response_processor=process_write_email_response,\n",
    "        description=\"Email sending requires approval\"\n",
    "    ),\n",
    "    \"schedule_meeting\": ToolInterruptConfig(\n",
    "        payload_builder=build_schedule_meeting_payload,\n",
    "        response_processor=process_schedule_meeting_response,\n",
    "        description=\"Meeting scheduling requires approval\"\n",
    "    ),\n",
    "    \"Question\": ToolInterruptConfig(\n",
    "        payload_builder=build_question_payload,\n",
    "        response_processor=process_question_response,\n",
    "        description=\"Question requires user answer\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74260be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes \n",
    "from typing import Literal\n",
    "from ambient_email_agent.prompts import default_triage_instructions, triage_system_prompt, triage_user_prompt\n",
    "from ambient_email_agent.schemas import RouterSchema, State\n",
    "from ambient_email_agent.utils import format_email_markdown, parse_email\n",
    "from langgraph.graph import END\n",
    "from langgraph.types import Command\n",
    "\n",
    "router_llm = model_llama_groq.with_structured_output(RouterSchema)\n",
    "\n",
    "def triage_router(state: State) -> Command[Literal[\"triage_interrupt_handler\", \"response_agent\", END]]:\n",
    "    \"\"\"Analyze email content to decide if we should respond, notify, or ignore.\n",
    "\n",
    "    The triage step prevents the assistant from wasting time on:\n",
    "    - Marketing emails and spam\n",
    "    - Company-wide announcements\n",
    "    - Messages meant for other teams\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the email input\n",
    "    author, to, subject, email_thread = parse_email(state[\"email_input\"])\n",
    "    user_prompt = triage_user_prompt.format(\n",
    "        author=author, to=to, subject=subject, email_thread=email_thread\n",
    "    )\n",
    "\n",
    "    # Create email markdown for Agent Inbox in case of notification  \n",
    "    email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "\n",
    "    # Format system prompt with background and triage instructions\n",
    "    system_prompt = triage_system_prompt.format(\n",
    "        background=default_background,\n",
    "        triage_instructions=default_triage_instructions\n",
    "    )\n",
    "\n",
    "    # Run the router LLM\n",
    "    result = router_llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Decision\n",
    "    classification = result.classification\n",
    "\n",
    "    # Process the classification decision\n",
    "    if classification == \"respond\":\n",
    "        print(\"ðŸ“§ Classification: RESPOND - This email requires a response\")\n",
    "        # Next node\n",
    "        goto = \"response_agent\"\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": result.classification,\n",
    "            \"messages\": [{\"role\": \"user\",\n",
    "                            \"content\": f\"Respond to the email: {email_markdown}\"\n",
    "                        }],\n",
    "        }\n",
    "    elif classification == \"ignore\":\n",
    "        print(\"ðŸš« Classification: IGNORE - This email can be safely ignored\")\n",
    "\n",
    "        # Next node\n",
    "        goto = END\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": classification,\n",
    "        }\n",
    "\n",
    "    elif classification == \"notify\":\n",
    "        print(\"ðŸ”” Classification: NOTIFY - This email contains important information\") \n",
    "\n",
    "        # Next node\n",
    "        goto = \"triage_interrupt_handler\"\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": classification,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid classification: {classification}\")\n",
    "    return Command(goto=goto, update=update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt\n",
    "\n",
    "\n",
    "def triage_interrupt_handler(state: State) -> Command[Literal[\"response_agent\", END]]:\n",
    "    \"\"\"Handles interrupts from the triage step\"\"\"\n",
    "    \n",
    "    # Parse the email input\n",
    "    author, to, subject, email_thread = parse_email(state[\"email_input\"])\n",
    "\n",
    "    # Create email markdown for Agent Inbox in case of notification  \n",
    "    email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "\n",
    "    # Create messages\n",
    "    messages = [{\"role\": \"user\",\n",
    "                \"content\": f\"Email to notify user about: {email_markdown}\"\n",
    "                }]\n",
    "\n",
    "    # Create interrupt for Agent Inbox\n",
    "    request = {\n",
    "        \"action_request\": {\n",
    "            \"action\": f\"Email Assistant: {state['classification_decision']}\",\n",
    "            \"args\": {}\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"allow_ignore\": True,  \n",
    "            \"allow_respond\": True, \n",
    "            \"allow_edit\": False, \n",
    "            \"allow_accept\": False,  \n",
    "        },\n",
    "        # Email to show in Agent Inbox\n",
    "        \"description\": email_markdown,\n",
    "    }\n",
    "\n",
    "    # Agent Inbox responds with a list  \n",
    "    response = interrupt([request])[0]\n",
    "\n",
    "    # If user provides feedback, go to response agent and use feedback to respond to email   \n",
    "    if response[\"type\"] == \"response\":\n",
    "        # Add feedback to messages \n",
    "        user_input = response[\"args\"]\n",
    "        # Used by the response agent\n",
    "        messages.append({\"role\": \"user\",\n",
    "                        \"content\": f\"User wants to reply to the email. Use this feedback to respond: {user_input}\"\n",
    "                        })\n",
    "        # Go to response agent\n",
    "        goto = \"response_agent\"\n",
    "\n",
    "    # If user ignores email, go to END\n",
    "    elif response[\"type\"] == \"ignore\":\n",
    "        goto = END\n",
    "\n",
    "    # Catch all other responses\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid response: {response}\")\n",
    "\n",
    "    # Update the state \n",
    "    update = {\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "    return Command(goto=goto, update=update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab001836",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [write_email, schedule_meeting,check_calendar_availability,Question]\n",
    "tools_by_name = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98def77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambient_email_agent.schemas import State\n",
    "from ambient_email_agent.utils import extract_email_context\n",
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "response_agent = create_agent(\n",
    "    model=model_gemini_flash,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt_for_email_writer_agent,\n",
    "    state_schema=State,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[ModelCallLimitMiddleware(\n",
    "        thread_limit=20, run_limit=10, exit_behavior='error'\n",
    "    ), CustomInterruptMiddleware(\n",
    "        tool_configs=tool_configs,\n",
    "        tools_by_name=tools_by_name,\n",
    "        state_extractor=extract_email_context,\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "\n",
    "overall_workflow = (StateGraph(State)\n",
    ".add_node(\"triage_router\",triage_router)\n",
    ".add_node(\"triage_interrupt_handler\",triage_interrupt_handler)\n",
    ".add_node(\"response_agent\", response_agent)\n",
    ".add_edge(START, \"triage_router\")).compile(checkpointer=InMemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ecddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambient_email_agent.utils import show_graph\n",
    "show_graph(overall_workflow, xray=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
