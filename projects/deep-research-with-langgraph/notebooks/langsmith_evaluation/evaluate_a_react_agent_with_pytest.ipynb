{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1ffc26",
   "metadata": {},
   "source": [
    "## Test a ReAct agent with Pytest and LangSmith\n",
    "\n",
    "We will create a ReAct Agent tha answers questions about publicly traded stocks and write a comprehensive test suite for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7442b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM options\n",
    "from langchain.chat_models import init_chat_model\n",
    "model_gemini_flash = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", timeout=30, temperature=0)\n",
    "model_llama_groq = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\", timeout=30, temperature=0)\n",
    "model_gpt_4o_mini = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", timeout=30, temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f61af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, Literal, TypedDict\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from e2b_code_interpreter import Sandbox\n",
    "from langchain_community.utilities.polygon import PolygonAPIWrapper\n",
    "from langchain_community.tools.polygon.aggregates import PolygonAggregates\n",
    "# Define search tool\n",
    "search_tool = TavilySearchResults(\n",
    "  max_results=5,\n",
    "  include_raw_content=True,\n",
    ")\n",
    "\n",
    "# Define code tool\n",
    "def code_tool(code: str) -> str:\n",
    "  \"\"\"Execute python code and return the result.\"\"\"\n",
    "  sbx = Sandbox()\n",
    "  execution = sbx.run_code(code)\n",
    "\n",
    "  if execution.error:\n",
    "      return f\"Error: {execution.error}\"\n",
    "  return f\"Results: {execution.results}, Logs: {execution.logs}\"\n",
    "\n",
    "\n",
    "# Define input schema for stock ticker tool\n",
    "class TickerToolInput(TypedDict):\n",
    "  \"\"\"Input format for the ticker tool.\n",
    "    The tool will pull data in aggregate blocks (timespan_multiplier * timespan) from the from_date to the to_date\n",
    "  \"\"\"\n",
    "  ticker: Annotated[str, ..., \"The ticker symbol of the stock\"]\n",
    "  timespan: Annotated[Literal[\"minute\", \"hour\", \"day\", \"week\", \"month\", \"quarter\", \"year\"], ..., \"The size of the time window.\"]\n",
    "  timespan_multiplier: Annotated[int, ..., \"The multiplier for the time window\"]\n",
    "  from_date: Annotated[str, ..., \"The date to start pulling data from, YYYY-MM-DD format - ONLY include the year month and day\"]\n",
    "  to_date: Annotated[str, ..., \"The date to stop pulling data, YYYY-MM-DD format - ONLY include the year month and day\"]\n",
    "\n",
    "api_wrapper = PolygonAPIWrapper()\n",
    "polygon_aggregates = PolygonAggregates(api_wrapper=api_wrapper)\n",
    "\n",
    "# Define stock ticker tool\n",
    "def ticker_tool(query: TickerToolInput) -> str:\n",
    "  \"\"\"Pull data for the ticker.\"\"\"\n",
    "  return polygon_aggregates.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f6a6b",
   "metadata": {},
   "source": [
    "### Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abe0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "class AgentOutputFormat(TypedDict):\n",
    "    numeric_answer: Annotated[float | None, ..., \"The numeric answer, if the user asked for one\"]\n",
    "    text_answer: Annotated[str | None, ..., \"The text answer, if the user asked for one\"]\n",
    "    reasoning: Annotated[str, ..., \"The reasoning behind the answer\"]\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model_gemini_flash,\n",
    "    tools=[code_tool, search_tool, polygon_aggregates],\n",
    "    response_format=AgentOutputFormat,\n",
    "    system_prompt=\"You are a financial expert. Respond to the users query accurately\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e1af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "response = agent.invoke({\"messages\":HumanMessage(\"Hello ! How are you doing?\")})\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\":HumanMessage(\"What was the price of  Apple stock one month back?\")})\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf0ed8",
   "metadata": {},
   "source": [
    "### Test 1: No tool should get called for off-topic chatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083258c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import testing as t\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "  # <-- Can still use all normal pytest markers\n",
    "  \"query\",\n",
    "  [\"Hello!\", \"How are you doing?\"],\n",
    ")\n",
    "def test_no_tools_on_offtopic_query(query: str) -> None:\n",
    "  \"\"\"Test that the agent does not use tools on offtopic queries.\"\"\"\n",
    "  # Log the test example\n",
    "  t.log_inputs({\"query\": query})\n",
    "  expected = []\n",
    "  t.log_reference_outputs({\"tool_calls\": expected})\n",
    "  # Call the agent's model node directly instead of running the ReACT loop.\n",
    "  result = agent.nodes[\"agent\"].invoke(\n",
    "      {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "  )\n",
    "  #this for loop is just for activity tracing in notebook cell output\n",
    "  for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n",
    "  actual = result[\"messages\"][0].tool_calls\n",
    "  t.log_outputs({\"tool_calls\": actual})\n",
    "  # Check that no tool calls were made.\n",
    "  assert actual == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00214771",
   "metadata": {},
   "source": [
    "### Test 2: Simple Tool Calling\n",
    "\n",
    "For tool calling we are going to verify that the agent calls the correct tool with the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.langsmith\n",
    "def test_searches_for_correct_ticker() -> None:\n",
    "    \"\"\"Test that the model looks up the correct ticker on simple query.\"\"\"\n",
    "    # Log the test example\n",
    "    query = \"What is the price of Apple?\"\n",
    "    t.log_inputs({\"query\": query})\n",
    "    expected = \"AAPL\"\n",
    "    t.log_reference_outputs({\"ticker\": expected})\n",
    "    # Call the agent's model node directly instead of running the full ReACT loop.\n",
    "    result = agent.nodes[\"agent\"].invoke(\n",
    "      {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "    )\n",
    "\n",
    "    #this for loop is just for activity tracing in notebook cell output\n",
    "    for msg in result[\"messages\"]:\n",
    "        msg.pretty_print()\n",
    "\n",
    "    tool_calls = result[\"messages\"][0].tool_calls\n",
    "\n",
    "    if tool_calls[0][\"name\"] == polygon_aggregates.name:\n",
    "      actual = tool_calls[0][\"args\"][\"ticker\"]\n",
    "\n",
    "    else:\n",
    "      actual = None\n",
    "\n",
    "    # Check that the right ticker was queried\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8797306",
   "metadata": {},
   "source": [
    "### Test 3 : Complex tool calling\n",
    "\n",
    "With the coding tool, the inputs and outputs of the tool are much less constrained, and there are lots of ways to get to the right answer. In this case, it’s simpler to test that the tool is used correctly by running the full agent and asserting that it both calls the coding tool and that it ends up with the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ab816",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.langsmith\n",
    "def test_executes_code_when_needed() -> None:\n",
    "    query = (\n",
    "      \"In the past year Facebook stock went up by 66.76%, \"\n",
    "      \"Apple by 25.24%, Google by 37.11%, Amazon by 47.52%, \"\n",
    "      \"Netflix by 78.31%. Whats the avg return in the past \"\n",
    "      \"year of the FAANG stocks, expressed as a percentage?\"\n",
    "    )\n",
    "\n",
    "    t.log_inputs({\"query\": query})\n",
    "    expected = 50.988\n",
    "    t.log_reference_outputs({\"response\": expected})\n",
    "    # Test that the agent executes code when needed\n",
    "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "    t.log_outputs({\"result\": result[\"structured_response\"].get(\"numeric_answer\")})\n",
    "    # Grab all the tool calls made by the LLM\n",
    "    tool_calls = [\n",
    "      tc[\"name\"]\n",
    "      for msg in result[\"messages\"]\n",
    "      for tc in getattr(msg, \"tool_calls\", [])\n",
    "    ]\n",
    "\n",
    "    # This will log the number of steps taken by the agent, which is useful for\n",
    "    # determining how efficiently the agent gets to an answer.\n",
    "    t.log_feedback(key=\"num_steps\", score=len(result[\"messages\"]) - 1)\n",
    "    # Assert that the code tool was used\n",
    "    assert \"code_tool\" in tool_calls\n",
    "    # Assert that a numeric answer was provided:\n",
    "    assert result[\"structured_response\"].get(\"numeric_answer\") is not None\n",
    "    # Assert that the answer is correct\n",
    "    assert abs(result[\"structured_response\"][\"numeric_answer\"] - expected) <= 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf909ab",
   "metadata": {},
   "source": [
    "### Test 4: LLM-as-judge\n",
    "\n",
    "We are going to ensure that the agent’s answer is grounded in the search results by running an LLM-as-a-judge evaluation. In order to trace the LLM as a judge call separately from our agent, we will use the LangSmith provided trace_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8185a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class Grade(TypedDict):\n",
    "  \"\"\"Evaluate the groundedness of an answer in source documents.\"\"\"\n",
    "  score: Annotated[\n",
    "      bool,\n",
    "      ...,\n",
    "      \"Return True if the answer is fully grounded in the source documents, otherwise False.\",\n",
    "  ]\n",
    "\n",
    "judge_llm = model_gemini_flash.with_structured_output(Grade)\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "def test_grounded_in_source_info() -> None:\n",
    "    \"\"\"Test that response is grounded in the tool outputs.\"\"\"\n",
    "    query = \"How did Nvidia stock do in 2024 according to analysts?\"\n",
    "    t.log_inputs({\"query\": query})\n",
    "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "    # Grab all the search calls made by the LLM\n",
    "    search_results = \"\\n\\n\".join(\n",
    "      msg.content\n",
    "      for msg in result[\"messages\"]\n",
    "      if msg.type == \"tool\" and msg.name == search_tool.name\n",
    "    )\n",
    "    t.log_outputs(\n",
    "      {\n",
    "          \"response\": result[\"structured_response\"].get(\"text_answer\"),\n",
    "          \"search_results\": search_results,\n",
    "      }\n",
    "    )\n",
    "    # Trace the feedback LLM run separately from the agent run.\n",
    "    with t.trace_feedback():\n",
    "      # Instructions for the LLM judge\n",
    "      instructions = (\n",
    "          \"Grade the following ANSWER. \"\n",
    "          \"The ANSWER should be fully grounded in (i.e. supported by) the source DOCUMENTS. \"\n",
    "          \"Return True if the ANSWER is fully grounded in the DOCUMENTS. \"\n",
    "          \"Return False if the ANSWER is not grounded in the DOCUMENTS.\"\n",
    "      )\n",
    "      answer_and_docs = (\n",
    "          f\"ANSWER: {result['structured_response'].get('text_answer', '')}\\n\"\n",
    "          f\"DOCUMENTS:\\n{search_results}\"\n",
    "      )\n",
    "      # Run the judge LLM\n",
    "      grade = judge_llm.invoke(\n",
    "          [\n",
    "              {\"role\": \"system\", \"content\": instructions},\n",
    "              {\"role\": \"user\", \"content\": answer_and_docs},\n",
    "          ]\n",
    "      )\n",
    "      t.log_feedback(key=\"groundedness\", score=grade[\"score\"])\n",
    "    assert grade['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c31d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
