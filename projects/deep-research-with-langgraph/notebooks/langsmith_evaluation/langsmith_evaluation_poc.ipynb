{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2578c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM for use with router / structured output\n",
    "from langchain.chat_models import init_chat_model\n",
    "model_gemini_flash = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", timeout=30, temperature=0)\n",
    "model_llama_groq = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\", timeout=30, temperature=0)\n",
    "model_gpt_4o_mini = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", timeout=30, temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langsmith import traceable\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "  result:Literal[\"TOXIC\",\"NOT_TOXIC\"] = Field(\"llm's classification result if the user input text is 'TOXIC' or 'NON_TOXIC'\")\n",
    "\n",
    "@traceable\n",
    "def toxicity_classifier(input:dict)->dict:\n",
    "    instructions = (\n",
    "      \"Please review the user query below and determine if it contains any form of toxic behavior, \"\n",
    "      \"such as insults, threats, or highly negative comments. Respond with 'TOXIC' if it does \"\n",
    "      \"and 'NOT_TOXIC' if it doesn't.\"\n",
    "    )\n",
    "    classification_result = model_llama_groq.with_structured_output(ClassificationResult).invoke([SystemMessage(instructions), HumanMessage(input[\"text\"])])\n",
    "\n",
    "    return {\"class\":classification_result.result}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747946a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_classifier({\"text\": \"This is unacceptable. I want to speak to the manager.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "ls_client = Client()\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"inputs\": {\"text\": \"Shut up, idiot\"},\n",
    "    \"outputs\": {\"label\": \"TOXIC\"},\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"text\": \"You're a wonderful person\"},\n",
    "    \"outputs\": {\"label\": \"NOT_TOXIC\"},\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"text\": \"This is the worst thing ever\"},\n",
    "    \"outputs\": {\"label\": \"TOXIC\"},\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"text\": \"I had a great day today\"},\n",
    "    \"outputs\": {\"label\": \"NOT_TOXIC\"},\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"text\": \"Nobody likes you\"},\n",
    "    \"outputs\": {\"label\": \"TOXIC\"},\n",
    "  },\n",
    "  {\n",
    "    \"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\n",
    "    \"outputs\": {\"label\": \"NOT_TOXIC\"},\n",
    "  },\n",
    "]\n",
    "\n",
    "dataset_name=\"Toxic Queries\"\n",
    "if not ls_client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = ls_client.create_dataset(dataset_name=dataset_name)\n",
    "    ls_client.create_examples(\n",
    "        dataset_id=dataset.id,\n",
    "        examples=examples,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8656aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(inputs:dict, outputs:dict, reference_outputs:dict)->bool:\n",
    "    return outputs[\"class\"] == reference_outputs[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ls_client.evaluate(\n",
    "    toxicity_classifier,\n",
    "    data=dataset.name,\n",
    "    evaluators=[correct],\n",
    "    experiment_prefix=\"llama-3.1-8b-instant, groq\",  # optional, experiment name prefix\n",
    "    description=\"Testing the baseline system.\",  # optional, experiment description\n",
    "    max_concurrency=4,  # optional, add concurrency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3a98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
