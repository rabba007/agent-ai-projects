{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9af3492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958dc3e2",
   "metadata": {},
   "source": [
    "### Find Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48726258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: https://www.perplexity.ai\n",
      "Perplexity Blog: https://www.perplexity.ai/hub\n",
      "Browse at the speed of thought: https://www.perplexity.ai/comet/\n",
      "Perplexity - AI Search & Chat - App Store - Apple: https://apps.apple.com/us/app/perplexity-ai-search-chat/id1668000334\n",
      "Changelog: https://docs.perplexity.ai/changelog/changelog\n",
      "Comet Browser Review 2026: Features, Pricing, Pros & Cons: https://efficient.app/apps/comet\n",
      "Perplexity - AI Companion - Chrome Web Store: https://chromewebstore.google.com/detail/perplexity-ai-companion/hlgbcneanomplepojfcnclggenpcoldo\n",
      "Perplexity Release Notes - December 2025 Latest Updates: https://releasebot.io/updates/perplexity-ai\n",
      "Introducing Comet: Browse at the speed of thought: https://www.perplexity.ai/hub/blog/introducing-comet\n",
      "Perplexity AI - Wikipedia: https://en.wikipedia.org/wiki/Perplexity_AI\n"
     ]
    }
   ],
   "source": [
    "from perplexity import Perplexity\n",
    "\n",
    "client = Perplexity()\n",
    "\n",
    "search = client.search.create(\n",
    "    query=[\n",
    "      \"What is Comet Browser?\",\n",
    "      \"Perplexity AI\",\n",
    "      \"Perplexity Changelog\"\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c87b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: https://www.perplexity.ai\n",
      "Perplexity Blog: https://www.perplexity.ai/hub\n",
      "Browse at the speed of thought: https://www.perplexity.ai/comet/\n",
      "Perplexity - AI Search & Chat - App Store - Apple: https://apps.apple.com/us/app/perplexity-ai-search-chat/id1668000334\n",
      "Changelog: https://docs.perplexity.ai/changelog/changelog\n",
      "Comet Browser Review 2026: Features, Pricing, Pros & Cons: https://efficient.app/apps/comet\n",
      "Perplexity - AI Companion - Chrome Web Store: https://chromewebstore.google.com/detail/perplexity-ai-companion/hlgbcneanomplepojfcnclggenpcoldo\n",
      "Perplexity Release Notes - December 2025 Latest Updates: https://releasebot.io/updates/perplexity-ai\n",
      "Introducing Comet: Browse at the speed of thought: https://www.perplexity.ai/hub/blog/introducing-comet\n",
      "Perplexity AI - Wikipedia: https://en.wikipedia.org/wiki/Perplexity_AI\n"
     ]
    }
   ],
   "source": [
    "for result in search.results:\n",
    "    print(f\"{result.title}: {result.url}\")\n",
    "    # if result.snippet:\n",
    "    #     print(f\"Snippet : \\n {result.snippet}\\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e065991",
   "metadata": {},
   "source": [
    "### Chat with Grounded Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67623d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on today's announcements, several significant AI developments have emerged across healthcare, enterprise, and climate sectors:\n",
      "\n",
      "**Healthcare AI Expansion**: Anthropic launched **Claude for Healthcare**, enabling U.S. subscribers to securely connect lab results and health records through integrations with HealthEx and Function, with Apple Health and Android Health Connect support rolling out this week[2]. The platform can summarize medical history, explain test results in plain language, and detect health patterns to help patients prepare for doctor appointments[2]. This follows OpenAI's recent launch of ChatGPT Health, intensifying competition in AI-assisted healthcare[2].\n",
      "\n",
      "**Biotech and Pharma Focus**: AlphaSense is sponsoring Fierce JPM Week, a major healthcare and biotech conference, where industry leaders are discussing AI's role in offsetting drug development costs and timelines[1]. A panel discussion titled \"Beyond the Lab: How AI is Reshaping Portfolio Strategy, Capital Allocation, and R&D Investment in Biopharma\" will feature executives from AbbVie, Evidation, GSK, and Zoetis, focusing on how AI is being applied to portfolio strategy and R&D investment decisions[1].\n",
      "\n",
      "**Retail AI Integration**: Kyndryl's 2025 Retail Readiness Report reveals that 89% of retail leaders believe AI will transform retail job roles within 12 months, though retailers face challenges with data silos and technical debt blocking agentic AI adoption[5].\n",
      "\n",
      "**Climate and Commerce**: The IPCC is hiring an Artificial Intelligence Officer to integrate AI tools into climate assessment processes, including literature reviews and text refinement[6]. Google is also launching a new platform for AI-powered shopping agents, advancing its commerce AI strategy[8].\n",
      "\n",
      "These announcements reflect AI's expanding role across healthcare decision-making, drug discovery, retail operations, and scientific research.\n"
     ]
    }
   ],
   "source": [
    "from perplexity import Perplexity\n",
    "\n",
    "client = Perplexity()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the major AI developments and announcements from today across the tech industry?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"sonar\"\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81addc",
   "metadata": {},
   "source": [
    "### Filter your sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adad09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! extra_body is not a default parameter.\n",
      "                    extra_body was transferred to model_kwargs.\n",
      "                    Please confirm that extra_body is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent arXiv publications in computer vision and multimodal AI highlight breakthroughs in **3D representation learning from videos, multimodal integration, context-aware modeling, real-time processing, and generative frameworks**, primarily from late 2025 papers summarized in AI Frontiers analyses.[1][2][3][4]\n",
      "\n",
      "### Key Computer Vision Breakthroughs\n",
      "- **LAM3C framework** learns 3D representations from ordinary internet videos without expensive scanning, outperforming scan-based methods and democratizing 3D understanding.[1]\n",
      "- **Geo-semantic scene graphs** boost object classification accuracy from 38% to 73% via spatial context modeling, surpassing larger models without scaling.[1]\n",
      "- **Video-BrowseComp benchmark** exposes gaps in agentic video research, with even GPT-5.1 at only 15% accuracy, advancing active multimodal exploration.[1]\n",
      "- **Image tiling strategies** recover fine-grained details in high-resolution processing while maintaining efficiency, with task-dependent optimizations balancing local and global context.[2]\n",
      "- **MorphoSim** generates controllable 3D environments from natural language for robotics training, enabling real-time editable scenes.[3]\n",
      "- **Transparent object perception** repurposes video diffusion models for breakthroughs in challenging domains like remote sensing and surgery.[4]\n",
      "- **Streaming video super-resolution** achieves 130x latency reduction, alongside synthetic CT from MRI with 99% structural similarity for radiation-free imaging.[4]\n",
      "\n",
      "### Promising Multimodal AI Advances\n",
      "- **Multimodal integration** combines vision with language, breaking sensory silos; themes include real-time edge deployment, 3D spatial reasoning, and video-language fusion.[1][2][3]\n",
      "- **VideoAR** uses autoregressive next-frame and scale prediction for video generation, enhancing multimodal generation capabilities.[6]\n",
      "- **Visual reasoning via reinforcement learning** improves multimodal models' insight from sight, as in \"From Sight to Insight,\" with extensive experiments across 23 pages.[7]\n",
      "- **Itemized text supervision** and cross-sample collaboration yield complete visual representations and higher-quality image generation.[2]\n",
      "- **Modular architectures** separate reasoning from prediction to cut hallucinations in multimodal systems.[2]\n",
      "\n",
      "### Dominant Themes Across Papers\n",
      "These advances span **six recurring themes**: multimodal fusion, foundation models for accessibility, self-supervised video learning, real-time adaptation, context boosting accuracy over scaling, and domain-specific applications (e.g., medical, autonomous systems).[1][2][3][4] Attention mechanisms and transformers capture long-range visual dependencies, with robustness under severity level 5 conditions (e.g., 8.3% mean error on CF10C).[5] Recent listings confirm ongoing momentum in video generation and detection (e.g., arXiv:2601.05143, 2601.05966).[6][7] Limitations include benchmark gaps and domain-specific scaling, but trends point to interpretable, deployable AI for human collaboration.[2][4]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"sonar\",\n",
    "    model_provider=\"perplexity\",\n",
    "    extra_body={\n",
    "        \"web_search_options\": {\n",
    "            \"search_domain_filter\": [\"arxiv.org\"],\n",
    "            \"search_recency_filter\": \"month\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"What are the most promising machine learning breakthroughs in computer vision and multimodal AI from recent arXiv publications?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "completion = model.invoke(messages)\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c25fb",
   "metadata": {},
   "source": [
    "### Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Startup(BaseModel):\n",
    "    \"\"\"Single startup info\"\"\"\n",
    "    company_name: str = Field(..., description=\"Company name\")\n",
    "    funding_amount: str = Field(..., description=\"Funding amount\")\n",
    "    focus_area: str = Field(..., description=\"Focus area\")\n",
    "\n",
    "class StartupsResponse(BaseModel):\n",
    "    \"\"\"List of startups\"\"\"\n",
    "    startups: List[Startup] = Field(..., description=\"List of startups\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\"Find the top 3 trending AI startups with recent funding. Include company name, funding amount, and focus area.\")\n",
    "]\n",
    "model_with_structured_output = init_chat_model(\n",
    "    \"sonar\",\n",
    "    model_provider=\"perplexity\").with_structured_output(StartupsResponse)\n",
    "\n",
    "startupList = model_with_structured_output.invoke(messages)\n",
    "for idx, startup in enumerate(startupList.startups):\n",
    "    print(f\" {idx + 1}. {startup.company_name} Funding Amount : {startup.funding_amount} focus_area : {startup.focus_area}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
